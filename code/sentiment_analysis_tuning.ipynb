{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments\n",
    "\n",
    "You will vectorize the text (located in the column `text`) by using the well-known TF-IDF technique. There will be three cases where the vocabulary of `TfidfVectorizer` will be limited to:\n",
    "\n",
    "1. Contain words that appear in at least 5 documents (hint: `min_df` parameter of `TfidfVectorizer`).\n",
    "2. Contain 2500 words (hint: `max_features` parameter of `TfidfVectorizer`).\n",
    "3. Contain 500 words (hint: `max_features` parameter of `TfidfVectorizer`).\n",
    "\n",
    "The classifiers will be evaluated by using 5-fold cross validation. Make sure that no information will be leaked from the training set to the test set. The values of the four following metrics will be measured:\n",
    "\n",
    "* $M_1$: Accuracy\n",
    "* $M_2$: F1-score\n",
    "* $M_3$: Fit time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Please write your solution here, including your code and descriptions. **Do not modify the notebook's structure**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries for the project\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the twitter database from destination path\n",
    "data = pd.read_csv(r\"/Users/dimzografos/Desktop/Assignments/MLPC/Twitter_US_Airline_Sentiment.csv\")\n",
    "\n",
    "print(\"Dataframe shape:\", data.shape)\n",
    "\n",
    "# Display the first 10 rows of the dataframe\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total of missing values\n",
    "print(f\"Missing values:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all except relevant features\n",
    "data.drop(columns=[col for col in data.columns if col not in [\"text\", \"airline_sentiment\"]], inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data (run once if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocessor(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters and numbers\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all text data\n",
    "data['text'] = data['text'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode airline_sentiment column\n",
    "le = LabelEncoder()\n",
    "data['airline_sentiment'] = le.fit_transform(data['airline_sentiment'])\n",
    "\n",
    "data[['text', 'airline_sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and target\n",
    "X = data['text']\n",
    "y = data['airline_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42),\n",
    "    'SVM': LinearSVC(multi_class='ovr', max_iter=1000, random_state=42, dual = True),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=1000, random_state=42),\n",
    "    'Feed-forward Neural Network': MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=300, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define TD-IDF Vectorization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three TF-IDF vectorization settings\n",
    "tfidf = {\n",
    "    'min_df=5': TfidfVectorizer(min_df=5),\n",
    "    'max_features=2500': TfidfVectorizer(max_features=2500),\n",
    "    'max_features=500': TfidfVectorizer(max_features=500)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create Pipeline and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for each classifier\n",
    "def create_pipeline(classifier, tfidf_vectorizer):\n",
    "    return Pipeline([\n",
    "        ('tfidf', tfidf_vectorizer),\n",
    "        ('classifier', classifier)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pipeline using cross-validation\n",
    "def evaluate_pipeline(pipeline, X, y, kf, scoring):\n",
    "    \n",
    "    cv_results = cross_validate(pipeline, X, y, cv=kf, scoring=scoring, return_train_score=False)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': np.mean(cv_results['test_accuracy']),\n",
    "        'F1-score': np.mean(cv_results['test_f1_weighted']),\n",
    "        'Fit Time (s)': np.mean(cv_results['fit_time'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate all Classifiers and IF-IDF Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate all classifiers for each TF-IDF \n",
    "def evaluate_all_classifiers(X, y, tfidf, classifiers, kf):\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    scoring = {'accuracy': 'accuracy', 'f1_weighted': 'f1_weighted'}\n",
    "    \n",
    "    for setting_name, tfidf_vectorizer in tfidf.items():\n",
    "        print(f\"\\nTF-IDF: {setting_name} \\n\")\n",
    "        \n",
    "        results = {}\n",
    "        for name, clf in classifiers.items():\n",
    "            pipeline = create_pipeline(clf, tfidf_vectorizer)\n",
    "            metrics = evaluate_pipeline(pipeline, X, y, kf, scoring)\n",
    "            results[name] = metrics\n",
    "            \n",
    "            print(f\"{name}:\")\n",
    "            print(f\"Accuracy: {metrics['Accuracy']*100:.2f} %\")\n",
    "            print(f\"F1-score: {metrics['F1-score']*100:.2f} %\")\n",
    "            print(f\"Fit Time: {metrics['Fit Time (s)']:.2f} seconds\")\n",
    "            print()\n",
    "        \n",
    "        all_results[setting_name] = results\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(all_results):\n",
    "    \n",
    "    tfidf_names = list(all_results.keys())\n",
    "    classifier_names = list(classifiers.keys())\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    accuracy_data = {clf: [] for clf in classifier_names}\n",
    "    f1_data = {clf: [] for clf in classifier_names}\n",
    "    for setting in tfidf_names:\n",
    "        for clf in classifier_names:\n",
    "            accuracy_data[clf].append(all_results[setting][clf]['Accuracy'])\n",
    "            f1_data[clf].append(all_results[setting][clf]['F1-score'])\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for clf in classifier_names:\n",
    "        plt.plot(tfidf_names, accuracy_data[clf], marker='o', label=clf)\n",
    "    plt.title('Classifier Accuracy Across TF-IDF Settings')\n",
    "    plt.xlabel('TF-IDF Setting')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot F1-score\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for clf in classifier_names:\n",
    "        plt.plot(tfidf_names, f1_data[clf], marker='o', label=clf)\n",
    "    plt.title('Classifier F1-Score Across TF-IDF Settings')\n",
    "    plt.xlabel('TF-IDF Setting')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Run Experiments and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation\n",
    "all_results = evaluate_all_classifiers(X, y, tfidf, classifiers, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Plots\n",
    "plot_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"sentiment_analysis_no_tuning.csv\"\n",
    "folder_path = r\"/Users/dimzografos/Desktop/Assignments/MLPC\"\n",
    "full_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "data.to_csv(full_path, index=False)\n",
    "\n",
    "print(f\"File saved at: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>plus youve added commercial experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment                                               text\n",
       "0                  1                                               said\n",
       "1                  2       plus youve added commercial experience tacky\n",
       "2                  1       didnt today must mean need take another trip\n",
       "3                  0  really aggressive blast obnoxious entertainmen...\n",
       "4                  0                               really big bad thing"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned and encoded dataset\n",
    "data = pd.read_csv(r\"/Users/dimzografos/Desktop/Assignments/MLPC/preprocessed_airline_sentiment.csv\")\n",
    "\n",
    "# Read the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "data.dropna(subset=['text', 'airline_sentiment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all text values are strings\n",
    "data['text'] = data['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate feature and target\n",
    "X = data['text']\n",
    "y = data['airline_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Classifiers and Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': LinearSVC(max_iter=1000, dual='auto'),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Neural Network': MLPClassifier(max_iter=300)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for each classifier\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__solver': ['lbfgs']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'classifier__C': [0.1, 1, 10]  # Removed max_iter â€“ set during init\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [100, 300],\n",
    "        'classifier__max_depth': [None, 10, 30]\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'classifier__hidden_layer_sizes': [(100,), (50, 50)],\n",
    "        'classifier__activation': ['relu', 'tanh']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define TF-IDF Vectorization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = {\n",
    "    'min_df=5': TfidfVectorizer(min_df=5),\n",
    "    'max_features=2500': TfidfVectorizer(max_features=2500),\n",
    "    'max_features=500': TfidfVectorizer(max_features=500)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation and scoring\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      " TF-IDF Setting: min_df=5\n",
      "============================\n",
      "Tuning Logistic Regression...\n",
      "Tuning SVM...\n",
      "Tuning Random Forest...\n",
      "Tuning Neural Network...\n",
      "\n",
      "============================\n",
      " TF-IDF Setting: max_features=2500\n",
      "============================\n",
      "Tuning Logistic Regression...\n",
      "Tuning SVM...\n",
      "Tuning Random Forest...\n",
      "Tuning Neural Network...\n",
      "\n",
      "============================\n",
      " TF-IDF Setting: max_features=500\n",
      "============================\n",
      "Tuning Logistic Regression...\n",
      "Tuning SVM...\n",
      "Tuning Random Forest...\n",
      "Tuning Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store tuning results\n",
    "all_results = []\n",
    "\n",
    "# Loop through each TF-IDF configuration\n",
    "for tfidf_name, tfidf_vectorizer in tfidf.items():\n",
    "    print(f\"\\n============================\")\n",
    "    print(f\" TF-IDF Setting: {tfidf_name}\")\n",
    "    print(f\"============================\")\n",
    "\n",
    "    # Loop through each classifier and its parameter grid\n",
    "    for clf_name in classifiers:\n",
    "        print(f\"Tuning {clf_name}...\")\n",
    "\n",
    "        # Create pipeline: TF-IDF vectorizer + classifier\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', tfidf_vectorizer),\n",
    "            ('classifier', classifiers[clf_name])\n",
    "        ])\n",
    "\n",
    "        # Run Grid Search with 5-fold cross-validation\n",
    "        grid = GridSearchCV(pipeline, param_grids[clf_name], cv=kf, scoring=scorer, n_jobs=-1)\n",
    "        grid.fit(X, y)\n",
    "\n",
    "        # Extract the best model from grid search\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        # Evaluate best model using cross-validation (accuracy + F1-score)\n",
    "        acc = cross_val_score(best_model, X, y, cv=kf, scoring='accuracy').mean()\n",
    "        f1 = cross_val_score(best_model, X, y, cv=kf, scoring='f1_weighted').mean()\n",
    "\n",
    "        # Store results in a list of dictionaries\n",
    "        all_results.append({\n",
    "            'TF-IDF Setting': tfidf_name,\n",
    "            'Model': clf_name,\n",
    "            'Accuracy': round(acc, 4),\n",
    "            'F1-score': round(f1, 4),\n",
    "            'Best Parameters': grid.best_params_\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Export Tuned Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning results saved to: /Users/dimzografos/Desktop/Assignments/MLPC/sentiment_analysis_with_tuning.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Define output file path\n",
    "output_path = r\"/Users/dimzografos/Desktop/Assignments/MLPC/sentiment_analysis_with_tuning.csv\"\n",
    "\n",
    "# Save the tuning results\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nTuning results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
